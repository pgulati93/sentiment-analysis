{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re, string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to read data and tokenize text\n",
    "def read_data(file, colspecs, columns):\n",
    "    # Import train data from text to dataframe\n",
    "    df = pd.read_fwf(file, colspecs=colspecs, header=None, names=columns)\n",
    "    print(\"Text file converted to dataframe.\")\n",
    "\n",
    "    # Tokenize the text\n",
    "    '''\n",
    "    Tokenizing the text data converts it from a single string to a list of words\n",
    "    and punctuation.\n",
    "    '''\n",
    "    df[\"tokens\"] = df[\"text\"].apply(word_tokenize)\n",
    "    print(\"Text data tokenized.\\n\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to normalize text\n",
    "'''\n",
    "Normalizing the text will involve:\n",
    "- Removing special characters from words\n",
    "- Lowercasing words\n",
    "- Lemmatizing words by finding their root\n",
    "- POS-tagging words to understand their grammatical position,\n",
    "  and using this information to improve lemmatizing accuracy\n",
    "'''\n",
    "\n",
    "def normalize_text(word):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    normalized_sentence = []\n",
    "    printable = set(string.printable)\n",
    "\n",
    "    for word, tag in pos_tag(word):\n",
    "        word = ''.join(filter(lambda x: x in printable, word))\n",
    "        word = word.lower()\n",
    "\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        pos = tag_dict.get(tag[0].upper(), wordnet.NOUN) # noun is the default\n",
    "\n",
    "        word = lemmatizer.lemmatize(word,pos)\n",
    "\n",
    "        stop_words = stopwords.words('english')\n",
    "        if len(word) > 0 and word not in string.punctuation and word not in stop_words:\n",
    "            normalized_sentence.append(word)\n",
    "\n",
    "    return normalized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to extract the list of frequent words\n",
    "'''\n",
    "This function creates a list of distinct words, organized by\n",
    "frequency, with an option to limit the max selection of words.\n",
    "Frequent words will be known as \"features\" in this code.\n",
    "'''\n",
    "\n",
    "def fdist_text(tokens_normalized, max=False):\n",
    "    # Create list of all words in text\n",
    "    words_list = []\n",
    "    for l in tokens_normalized:\n",
    "        words_list.extend(l)\n",
    "\n",
    "    fdist = nltk.FreqDist(w.lower() for w in words_list)\n",
    "    if max:\n",
    "        fdist_list = list(fdist)[:max]\n",
    "    else:\n",
    "        fdist_list = list(fdist)\n",
    "    print(\"Created list of {} most frequent words\".format(len(fdist_list)))\n",
    "\n",
    "    return fdist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to check presence of feature words\n",
    "'''\n",
    "This function creates a list with a dictionary that detects the\n",
    "presence of features (aka frequent words) in each text.\n",
    "'''\n",
    "def features_identify(text):\n",
    "    text_lower = [w.lower() for w in text]\n",
    "    # \"set\" creates a set of unique words in each text\n",
    "    text_words = set(text_lower)\n",
    "    features = {}\n",
    "    for word in fdist_list:\n",
    "        features[word] = (word in text_words)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to create a tuple for more effecient analysis\n",
    "'''\n",
    "This function creates a tuple [(a,b)] where \"a\" is the text and\n",
    "\"b\" is the sentiment.\n",
    "'''\n",
    "def tuple_df(text_df, sentiment_df):\n",
    "    tupled_df = tuple(zip(list(text_df), list(sentiment_df)))\n",
    "    \n",
    "    return tupled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train Naive Bayes classifier\n",
    "def train_nb_classifier(df, train_val=False, test_val=False):\n",
    "    tupled_df = tuple_df(df[\"tokens\"], df[\"sentiment\"])\n",
    "    print(\"Created tuple of text and sentiment.\")\n",
    "    \n",
    "    featuresets = [(features_identify(t),s) for (t,s) in tupled_df]\n",
    "    random.shuffle(featuresets)\n",
    "    print(\"Created features-presence sets.\\n\")\n",
    "    \n",
    "    train_val = int(len(featuresets)/2) if train_val == False else train_val\n",
    "    test_val = int(len(featuresets)/2) if test_val == False else test_val\n",
    "    train_set, test_set = featuresets[train_val:], featuresets[:test_val]\n",
    "    \n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "    print(\"Created NB classifier.\")\n",
    "    print(\"Classifier accuracy is: {}\\n\".format(accuracy))\n",
    "    print(classifier.show_most_informative_features(5))\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to test the Naive Bayes classifier\n",
    "def test_nb_classifier(tokens):\n",
    "    tokens_normalized = normalize_text(tokens)\n",
    "    sentiment = classifier.classify(dict([t, True] for t in tokens_normalized))\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"training.txt\"\n",
    "train_colspecs = [(0,1), (2,None)] # specify column widths\n",
    "train_columns = [\"sentiment\", \"text\"] # specify column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file converted to dataframe.\n",
      "Text data tokenized.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data, convert to dataframe, and tokenize text\n",
    "train_df = read_data(train_file, train_colspecs, train_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized and cleaned up text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize and clean up tokenized text\n",
    "train_df[\"tokens_normalized\"] = train_df[\"tokens\"].apply(normalize_text)\n",
    "print(\"Normalized and cleaned up text.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created list of 1896 most frequent words\n"
     ]
    }
   ],
   "source": [
    "# Identify list of most frequent words\n",
    "fdist_list = fdist_text(train_df[\"tokens_normalized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created tuple of text and sentiment.\n",
      "Created features-presence sets.\n",
      "\n",
      "Created NB classifier.\n",
      "Classifier accuracy is: 0.9717753316398532\n",
      "\n",
      "Most Informative Features\n",
      "                 awesome = True                1 : 0      =    252.5 : 1.0\n",
      "                    love = True                1 : 0      =     90.4 : 1.0\n",
      "                   heard = True                0 : 1      =     51.0 : 1.0\n",
      "                  daniel = True                0 : 1      =     45.3 : 1.0\n",
      "                     soo = True                0 : 1      =     42.4 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Create classifier and report on accuracy\n",
    "classifier = train_nb_classifier(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file converted to dataframe.\n",
      "Text data tokenized.\n",
      "\n",
      "Classified sentiment of test data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import test data, and test classifier against this data\n",
    "test_file = \"testdata.txt\"\n",
    "test_colspecs = [(0, None)] # specify column widths\n",
    "test_columns = [\"text\"] # specify column names\n",
    "\n",
    "test_df = read_data(test_file, test_colspecs, test_columns)\n",
    "\n",
    "test_df[\"sentiment\"] = test_df[\"tokens\"].apply(test_nb_classifier)\n",
    "print(\"Classified sentiment of test data.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create text file of test data with classified sentiments\n",
    "test_file_classified = \"testdata_classified.txt\"\n",
    "test_columns_classified = [\"sentiment\",\"text\"]\n",
    "\n",
    "test_df.to_csv(test_file_classified, sep=\" \", columns=test_columns_classified, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
